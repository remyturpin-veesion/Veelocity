# v0.3 GitHub Actions & DORA Metrics Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Connect to GitHub Actions API, sync workflows and runs, calculate Deployment Frequency and Lead Time for Changes metrics, build basic Flutter dashboard.

**Architecture:** GitHubActionsConnector extends BaseConnector, shares the GitHub token. Workflow/WorkflowRun models store Actions data. MetricsService calculates DORA metrics from stored data. Flutter dashboard displays KPI cards.

**Tech Stack:** Same as v0.2 + Flutter Riverpod for state management, fl_chart for visualizations

---

## Task 1: Add GitHub Actions Models

**Files:**
- Modify: `backend/app/models/github.py`
- Create: `backend/alembic/versions/xxxx_add_github_actions_models.py`

**Step 1: Add Workflow and WorkflowRun models**

Add to `backend/app/models/github.py`:

```python
class Workflow(Base):
    __tablename__ = "workflows"

    id: Mapped[int] = mapped_column(primary_key=True)
    github_id: Mapped[int] = mapped_column(BigInteger, unique=True, index=True)
    repo_id: Mapped[int] = mapped_column(ForeignKey("repositories.id"))
    name: Mapped[str] = mapped_column(String(255))
    path: Mapped[str] = mapped_column(String(512))
    state: Mapped[str] = mapped_column(String(50))  # active, disabled, etc.
    is_deployment: Mapped[bool] = mapped_column(Boolean, default=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    repository: Mapped["Repository"] = relationship(back_populates="workflows")
    runs: Mapped[list["WorkflowRun"]] = relationship(back_populates="workflow")


class WorkflowRun(Base):
    __tablename__ = "workflow_runs"

    id: Mapped[int] = mapped_column(primary_key=True)
    github_id: Mapped[int] = mapped_column(BigInteger, unique=True, index=True)
    workflow_id: Mapped[int] = mapped_column(ForeignKey("workflows.id"))
    status: Mapped[str] = mapped_column(String(50))  # queued, in_progress, completed
    conclusion: Mapped[str | None] = mapped_column(String(50), nullable=True)  # success, failure, cancelled
    run_number: Mapped[int] = mapped_column(Integer)
    head_sha: Mapped[str] = mapped_column(String(40))
    head_branch: Mapped[str] = mapped_column(String(255))
    started_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    completed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    workflow: Mapped["Workflow"] = relationship(back_populates="runs")
```

**Step 2: Add relationships to Repository**

Update Repository model to add:

```python
workflows: Mapped[list["Workflow"]] = relationship(back_populates="repository")
```

**Step 3: Update models/__init__.py**

Add exports for Workflow and WorkflowRun.

**Step 4: Create Alembic migration**

Run: `cd backend && uv run alembic revision --autogenerate -m "add github actions models"`

**Step 5: Apply migration**

Run: `cd backend && uv run alembic upgrade head`

**Step 6: Commit**

```bash
git add backend/app/models/ backend/alembic/versions/
git commit -m "feat(backend): add Workflow and WorkflowRun models for GitHub Actions"
```

---

## Task 2: Add deployment pattern configuration

**Files:**
- Modify: `backend/app/core/config.py`

**Step 1: Add deployment detection patterns**

The config already has `deployment_patterns` but we need a helper to use it:

```python
# In config.py, add method to Settings class or as standalone function
def is_deployment_workflow(name: str, path: str) -> bool:
    """Check if workflow name/path matches deployment patterns."""
    patterns = settings.deployment_patterns.split(",")
    name_lower = name.lower()
    path_lower = path.lower()
    return any(
        pattern.strip().lower() in name_lower or pattern.strip().lower() in path_lower
        for pattern in patterns
    )
```

**Step 2: Commit**

```bash
git add backend/app/core/config.py
git commit -m "feat(backend): add is_deployment_workflow helper for pattern matching"
```

---

## Task 3: Create GitHubActionsConnector

**Files:**
- Create: `backend/app/connectors/github_actions.py`
- Modify: `backend/app/connectors/factory.py`
- Create: `backend/tests/connectors/test_github_actions.py`

**Step 1: Implement GitHubActionsConnector**

```python
# backend/app/connectors/github_actions.py
from datetime import datetime, timezone

import httpx

from app.connectors.base import BaseConnector
from app.core.config import settings, is_deployment_workflow
from app.schemas.connector import SyncResult


class GitHubActionsConnector(BaseConnector):
    """Connector for GitHub Actions API."""

    BASE_URL = "https://api.github.com"

    def __init__(self, token: str, repos: list[str]):
        self._token = token
        self._repos = repos
        self._client = httpx.AsyncClient(
            base_url=self.BASE_URL,
            headers={
                "Authorization": f"Bearer {token}",
                "Accept": "application/vnd.github.v3+json",
            },
            timeout=30.0,
        )

    @property
    def name(self) -> str:
        return "github_actions"

    def get_supported_metrics(self) -> list[str]:
        return ["deployment_frequency", "lead_time_for_changes"]

    async def test_connection(self) -> bool:
        response = await self._client.get("/user")
        return response.status_code == 200

    async def fetch_workflows(self, repo_full_name: str) -> list[dict]:
        """Fetch all workflows for a repository."""
        workflows = []
        page = 1
        while True:
            response = await self._client.get(
                f"/repos/{repo_full_name}/actions/workflows",
                params={"per_page": 100, "page": page},
            )
            if response.status_code != 200:
                break
            data = response.json()
            for wf in data.get("workflows", []):
                workflows.append({
                    "github_id": wf["id"],
                    "name": wf["name"],
                    "path": wf["path"],
                    "state": wf["state"],
                    "is_deployment": is_deployment_workflow(wf["name"], wf["path"]),
                })
            if len(data.get("workflows", [])) < 100:
                break
            page += 1
        return workflows

    async def fetch_workflow_runs(
        self, repo_full_name: str, workflow_id: int, per_page: int = 100, max_pages: int = 5
    ) -> list[dict]:
        """Fetch recent runs for a workflow. Limited to max_pages to avoid API limits."""
        runs = []
        page = 1
        while page <= max_pages:
            response = await self._client.get(
                f"/repos/{repo_full_name}/actions/workflows/{workflow_id}/runs",
                params={"per_page": per_page, "page": page},
            )
            if response.status_code != 200:
                break
            data = response.json()
            for run in data.get("workflow_runs", []):
                runs.append({
                    "github_id": run["id"],
                    "status": run["status"],
                    "conclusion": run.get("conclusion"),
                    "run_number": run["run_number"],
                    "head_sha": run["head_sha"],
                    "head_branch": run["head_branch"],
                    "started_at": run.get("run_started_at"),
                    "completed_at": run.get("updated_at") if run["status"] == "completed" else None,
                })
            if len(data.get("workflow_runs", [])) < per_page:
                break
            page += 1
        return runs

    async def sync_all(self, db) -> SyncResult:
        started_at = datetime.now(timezone.utc)
        items_synced = 0
        errors = []
        from app.services.sync_actions import SyncActionsService
        sync_service = SyncActionsService(db, self)
        try:
            items_synced = await sync_service.sync_all()
        except Exception as e:
            errors.append(str(e))
        return SyncResult(
            connector_name=self.name,
            started_at=started_at,
            completed_at=datetime.now(timezone.utc),
            items_synced=items_synced,
            errors=errors,
        )

    async def sync_recent(self, db, since: datetime) -> SyncResult:
        # For now, same as sync_all with limited pages
        return await self.sync_all(db)

    async def close(self):
        await self._client.aclose()
```

**Step 2: Update factory**

Add to `backend/app/connectors/factory.py`:

```python
from app.connectors.github_actions import GitHubActionsConnector

def create_github_actions_connector() -> GitHubActionsConnector | None:
    """Create GitHub Actions connector if configured."""
    if not settings.github_token:
        return None
    repos = [r.strip() for r in settings.github_repos.split(",") if r.strip()]
    if not repos:
        return None
    return GitHubActionsConnector(settings.github_token, repos)
```

**Step 3: Write tests with respx**

```python
# backend/tests/connectors/test_github_actions.py
import pytest
import respx
from httpx import Response

from app.connectors.github_actions import GitHubActionsConnector


@pytest.fixture
def connector():
    return GitHubActionsConnector(token="test-token", repos=["owner/repo"])


@pytest.mark.asyncio
@respx.mock
async def test_fetch_workflows(connector):
    respx.get("https://api.github.com/repos/owner/repo/actions/workflows").mock(
        return_value=Response(200, json={
            "workflows": [
                {"id": 1, "name": "CI", "path": ".github/workflows/ci.yml", "state": "active"},
                {"id": 2, "name": "Deploy Production", "path": ".github/workflows/deploy.yml", "state": "active"},
            ]
        })
    )
    
    workflows = await connector.fetch_workflows("owner/repo")
    
    assert len(workflows) == 2
    assert workflows[0]["name"] == "CI"
    assert workflows[0]["is_deployment"] is False
    assert workflows[1]["name"] == "Deploy Production"
    assert workflows[1]["is_deployment"] is True
    await connector.close()


@pytest.mark.asyncio
@respx.mock
async def test_fetch_workflow_runs(connector):
    respx.get("https://api.github.com/repos/owner/repo/actions/workflows/1/runs").mock(
        return_value=Response(200, json={
            "workflow_runs": [
                {
                    "id": 100,
                    "status": "completed",
                    "conclusion": "success",
                    "run_number": 42,
                    "head_sha": "abc123",
                    "head_branch": "main",
                    "run_started_at": "2026-01-20T10:00:00Z",
                    "updated_at": "2026-01-20T10:05:00Z",
                }
            ]
        })
    )
    
    runs = await connector.fetch_workflow_runs("owner/repo", 1)
    
    assert len(runs) == 1
    assert runs[0]["conclusion"] == "success"
    assert runs[0]["head_sha"] == "abc123"
    await connector.close()


@pytest.mark.asyncio
@respx.mock
async def test_test_connection_success(connector):
    respx.get("https://api.github.com/user").mock(return_value=Response(200, json={"login": "user"}))
    
    result = await connector.test_connection()
    
    assert result is True
    await connector.close()
```

**Step 4: Run tests**

Run: `cd backend && uv run pytest tests/connectors/test_github_actions.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/connectors/ backend/tests/connectors/
git commit -m "feat(backend): add GitHubActionsConnector for workflows and runs

- Fetch workflows with is_deployment detection
- Fetch workflow runs with pagination
- Shares GitHub token from existing config"
```

---

## Task 4: Create SyncActionsService

**Files:**
- Create: `backend/app/services/sync_actions.py`
- Create: `backend/tests/services/test_sync_actions.py`

**Step 1: Implement SyncActionsService**

```python
# backend/app/services/sync_actions.py
from datetime import datetime

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.connectors.github_actions import GitHubActionsConnector
from app.models.github import Repository, Workflow, WorkflowRun


def _parse_datetime(value: str | datetime | None) -> datetime | None:
    """Parse datetime string to naive datetime."""
    if value is None:
        return None
    if isinstance(value, str):
        dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
    else:
        dt = value
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt


class SyncActionsService:
    """Orchestrates sync of GitHub Actions data."""

    def __init__(self, db: AsyncSession, connector: GitHubActionsConnector):
        self._db = db
        self._connector = connector

    async def sync_all(self) -> int:
        """Full sync of workflows and runs."""
        count = 0
        
        # Get all repos from database
        result = await self._db.execute(select(Repository))
        repos = result.scalars().all()
        
        for repo in repos:
            workflows = await self._connector.fetch_workflows(repo.full_name)
            count += await self._upsert_workflows(repo.id, workflows)
            
            for wf_data in workflows:
                workflow = await self._get_workflow_by_github_id(wf_data["github_id"])
                if not workflow:
                    continue
                    
                runs = await self._connector.fetch_workflow_runs(repo.full_name, wf_data["github_id"])
                count += await self._upsert_runs(workflow.id, runs)
        
        await self._db.commit()
        return count

    async def _upsert_workflows(self, repo_id: int, workflows: list[dict]) -> int:
        count = 0
        for data in workflows:
            result = await self._db.execute(
                select(Workflow).where(Workflow.github_id == data["github_id"])
            )
            existing = result.scalar_one_or_none()
            wf_data = {**data, "repo_id": repo_id}
            if existing:
                for key, value in wf_data.items():
                    if key != "github_id":
                        setattr(existing, key, value)
            else:
                self._db.add(Workflow(**wf_data))
            count += 1
        await self._db.flush()
        return count

    async def _upsert_runs(self, workflow_id: int, runs: list[dict]) -> int:
        count = 0
        for data in runs:
            result = await self._db.execute(
                select(WorkflowRun).where(WorkflowRun.github_id == data["github_id"])
            )
            existing = result.scalar_one_or_none()
            run_data = {
                "github_id": data["github_id"],
                "workflow_id": workflow_id,
                "status": data["status"],
                "conclusion": data.get("conclusion"),
                "run_number": data["run_number"],
                "head_sha": data["head_sha"],
                "head_branch": data["head_branch"],
                "started_at": _parse_datetime(data.get("started_at")),
                "completed_at": _parse_datetime(data.get("completed_at")),
            }
            if existing:
                for key, value in run_data.items():
                    if key != "github_id":
                        setattr(existing, key, value)
            else:
                self._db.add(WorkflowRun(**run_data))
            count += 1
        await self._db.flush()
        return count

    async def _get_workflow_by_github_id(self, github_id: int) -> Workflow | None:
        result = await self._db.execute(
            select(Workflow).where(Workflow.github_id == github_id)
        )
        return result.scalar_one_or_none()
```

**Step 2: Write tests**

```python
# backend/tests/services/test_sync_actions.py
from unittest.mock import AsyncMock, MagicMock

import pytest

from app.services.sync_actions import SyncActionsService


@pytest.mark.asyncio
async def test_sync_all_calls_connector_methods():
    """Verify sync orchestration calls connector methods."""
    mock_db = MagicMock()
    mock_db.execute = AsyncMock(return_value=MagicMock(scalars=lambda: MagicMock(all=lambda: [])))
    mock_db.commit = AsyncMock()
    
    mock_connector = MagicMock()
    mock_connector.fetch_workflows = AsyncMock(return_value=[])
    mock_connector.fetch_workflow_runs = AsyncMock(return_value=[])
    
    service = SyncActionsService(mock_db, mock_connector)
    count = await service.sync_all()
    
    assert count == 0
    mock_db.commit.assert_called_once()
```

**Step 3: Run tests**

Run: `cd backend && uv run pytest tests/services/test_sync_actions.py -v`
Expected: PASS

**Step 4: Commit**

```bash
git add backend/app/services/sync_actions.py backend/tests/services/
git commit -m "feat(backend): add SyncActionsService for GitHub Actions data"
```

---

## Task 5: Add GitHub Actions API endpoints

**Files:**
- Create: `backend/app/api/v1/endpoints/actions.py`
- Modify: `backend/app/api/v1/router.py`
- Create: `backend/tests/api/test_actions.py`

**Step 1: Create actions endpoints**

```python
# backend/app/api/v1/endpoints/actions.py
from typing import Any

from fastapi import APIRouter, Depends
from sqlalchemy import func, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.core.database import get_db
from app.models.github import Workflow, WorkflowRun
from app.schemas.pagination import (
    PaginatedResponse,
    PaginationParams,
    get_pagination_params,
)

router = APIRouter(prefix="/actions", tags=["github-actions"])


@router.get("/workflows", response_model=PaginatedResponse[dict[str, Any]])
async def get_workflows(
    is_deployment: bool | None = None,
    pagination: PaginationParams = Depends(get_pagination_params),
    db: AsyncSession = Depends(get_db),
):
    """List all workflows, optionally filtered by is_deployment flag."""
    query = select(Workflow)
    count_query = select(func.count(Workflow.id))
    
    if is_deployment is not None:
        query = query.where(Workflow.is_deployment == is_deployment)
        count_query = count_query.where(Workflow.is_deployment == is_deployment)
    
    count_result = await db.execute(count_query)
    total = count_result.scalar() or 0
    
    result = await db.execute(
        query.order_by(Workflow.name)
        .offset(pagination.offset)
        .limit(pagination.limit)
    )
    workflows = result.scalars().all()
    
    items = [
        {
            "id": wf.id,
            "github_id": wf.github_id,
            "name": wf.name,
            "path": wf.path,
            "state": wf.state,
            "is_deployment": wf.is_deployment,
        }
        for wf in workflows
    ]
    return PaginatedResponse.create(items, total, pagination)


@router.get("/workflows/{workflow_id}/runs", response_model=PaginatedResponse[dict[str, Any]])
async def get_workflow_runs(
    workflow_id: int,
    conclusion: str | None = None,
    pagination: PaginationParams = Depends(get_pagination_params),
    db: AsyncSession = Depends(get_db),
):
    """List runs for a workflow, optionally filtered by conclusion."""
    query = select(WorkflowRun).where(WorkflowRun.workflow_id == workflow_id)
    count_query = select(func.count(WorkflowRun.id)).where(WorkflowRun.workflow_id == workflow_id)
    
    if conclusion is not None:
        query = query.where(WorkflowRun.conclusion == conclusion)
        count_query = count_query.where(WorkflowRun.conclusion == conclusion)
    
    count_result = await db.execute(count_query)
    total = count_result.scalar() or 0
    
    result = await db.execute(
        query.order_by(WorkflowRun.created_at.desc())
        .offset(pagination.offset)
        .limit(pagination.limit)
    )
    runs = result.scalars().all()
    
    items = [
        {
            "id": run.id,
            "github_id": run.github_id,
            "status": run.status,
            "conclusion": run.conclusion,
            "run_number": run.run_number,
            "head_sha": run.head_sha,
            "head_branch": run.head_branch,
            "started_at": run.started_at,
            "completed_at": run.completed_at,
        }
        for run in runs
    ]
    return PaginatedResponse.create(items, total, pagination)


@router.get("/deployments", response_model=PaginatedResponse[dict[str, Any]])
async def get_deployments(
    pagination: PaginationParams = Depends(get_pagination_params),
    db: AsyncSession = Depends(get_db),
):
    """List successful deployment runs (from workflows marked as deployment)."""
    query = (
        select(WorkflowRun)
        .join(Workflow)
        .where(Workflow.is_deployment == True)
        .where(WorkflowRun.conclusion == "success")
    )
    count_query = (
        select(func.count(WorkflowRun.id))
        .join(Workflow)
        .where(Workflow.is_deployment == True)
        .where(WorkflowRun.conclusion == "success")
    )
    
    count_result = await db.execute(count_query)
    total = count_result.scalar() or 0
    
    result = await db.execute(
        query.options(selectinload(WorkflowRun.workflow))
        .order_by(WorkflowRun.completed_at.desc())
        .offset(pagination.offset)
        .limit(pagination.limit)
    )
    runs = result.scalars().all()
    
    items = [
        {
            "id": run.id,
            "workflow_name": run.workflow.name,
            "run_number": run.run_number,
            "head_sha": run.head_sha,
            "head_branch": run.head_branch,
            "completed_at": run.completed_at,
        }
        for run in runs
    ]
    return PaginatedResponse.create(items, total, pagination)
```

**Step 2: Register router**

Add to `backend/app/api/v1/router.py`:

```python
from app.api.v1.endpoints import actions

api_router.include_router(actions.router)
```

**Step 3: Run tests**

Run: `cd backend && uv run pytest tests/api/ -v`
Expected: PASS

**Step 4: Commit**

```bash
git add backend/app/api/v1/
git commit -m "feat(backend): add GitHub Actions API endpoints

- GET /actions/workflows - list workflows with is_deployment filter
- GET /actions/workflows/{id}/runs - list runs with conclusion filter
- GET /actions/deployments - list successful deployment runs"
```

---

## Task 6: Create DORA Metrics Service

**Files:**
- Create: `backend/app/services/metrics/__init__.py`
- Create: `backend/app/services/metrics/dora.py`
- Create: `backend/tests/services/test_dora_metrics.py`

**Step 1: Create metrics package**

```python
# backend/app/services/metrics/__init__.py
from app.services.metrics.dora import DORAMetricsService

__all__ = ["DORAMetricsService"]
```

**Step 2: Implement DORA metrics calculations**

```python
# backend/app/services/metrics/dora.py
from datetime import datetime, timedelta
from typing import Literal

from sqlalchemy import and_, func, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.github import Commit, PullRequest, Workflow, WorkflowRun


class DORAMetricsService:
    """Calculate DORA metrics from synced data."""

    def __init__(self, db: AsyncSession):
        self._db = db

    async def get_deployment_frequency(
        self,
        start_date: datetime,
        end_date: datetime,
        period: Literal["day", "week", "month"] = "week",
        repo_id: int | None = None,
    ) -> dict:
        """
        Calculate deployment frequency over a period.
        
        Returns count of successful deployments grouped by period.
        """
        query = (
            select(WorkflowRun.completed_at)
            .join(Workflow)
            .where(
                and_(
                    Workflow.is_deployment == True,
                    WorkflowRun.conclusion == "success",
                    WorkflowRun.completed_at >= start_date,
                    WorkflowRun.completed_at <= end_date,
                )
            )
        )
        
        if repo_id:
            query = query.where(Workflow.repo_id == repo_id)
        
        result = await self._db.execute(query.order_by(WorkflowRun.completed_at))
        deployments = result.scalars().all()
        
        # Group by period
        grouped = self._group_by_period(deployments, period)
        
        # Calculate average
        total_deployments = len(deployments)
        period_count = self._count_periods(start_date, end_date, period)
        average = total_deployments / period_count if period_count > 0 else 0
        
        return {
            "period": period,
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "data": grouped,
            "total": total_deployments,
            "average": round(average, 2),
        }

    async def get_lead_time_for_changes(
        self,
        start_date: datetime,
        end_date: datetime,
        repo_id: int | None = None,
    ) -> dict:
        """
        Calculate lead time for changes.
        
        Lead time = time from first commit on PR to deployment containing that commit.
        
        Strategy:
        1. Get successful deployments in range
        2. For each deployment, find the PR that contains the deployed commit (head_sha)
        3. Find the first commit on that PR
        4. Lead time = deployment time - first commit time
        """
        # Get successful deployments
        deploy_query = (
            select(WorkflowRun)
            .join(Workflow)
            .where(
                and_(
                    Workflow.is_deployment == True,
                    WorkflowRun.conclusion == "success",
                    WorkflowRun.completed_at >= start_date,
                    WorkflowRun.completed_at <= end_date,
                )
            )
        )
        
        if repo_id:
            deploy_query = deploy_query.where(Workflow.repo_id == repo_id)
        
        result = await self._db.execute(deploy_query)
        deployments = result.scalars().all()
        
        lead_times = []
        
        for deployment in deployments:
            # Find commit by sha
            commit_result = await self._db.execute(
                select(Commit).where(Commit.sha == deployment.head_sha)
            )
            commit = commit_result.scalar_one_or_none()
            
            if not commit or not commit.pr_id:
                continue
            
            # Find first commit on the PR
            first_commit_result = await self._db.execute(
                select(Commit)
                .where(Commit.pr_id == commit.pr_id)
                .order_by(Commit.committed_at)
                .limit(1)
            )
            first_commit = first_commit_result.scalar_one_or_none()
            
            if first_commit and deployment.completed_at:
                lead_time = (deployment.completed_at - first_commit.committed_at).total_seconds()
                lead_times.append({
                    "deployment_id": deployment.id,
                    "first_commit_at": first_commit.committed_at.isoformat(),
                    "deployed_at": deployment.completed_at.isoformat(),
                    "lead_time_seconds": lead_time,
                    "lead_time_hours": round(lead_time / 3600, 2),
                })
        
        # Calculate statistics
        if lead_times:
            times = [lt["lead_time_hours"] for lt in lead_times]
            avg_hours = sum(times) / len(times)
            median_hours = sorted(times)[len(times) // 2]
        else:
            avg_hours = 0
            median_hours = 0
        
        return {
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "measurements": lead_times,
            "count": len(lead_times),
            "average_hours": round(avg_hours, 2),
            "median_hours": round(median_hours, 2),
        }

    def _group_by_period(
        self, dates: list[datetime], period: Literal["day", "week", "month"]
    ) -> list[dict]:
        """Group dates by period and count."""
        from collections import Counter
        
        def get_period_key(dt: datetime) -> str:
            if period == "day":
                return dt.strftime("%Y-%m-%d")
            elif period == "week":
                # ISO week
                return dt.strftime("%Y-W%W")
            else:  # month
                return dt.strftime("%Y-%m")
        
        counts = Counter(get_period_key(dt) for dt in dates if dt)
        return [{"period": k, "count": v} for k, v in sorted(counts.items())]

    def _count_periods(
        self, start: datetime, end: datetime, period: Literal["day", "week", "month"]
    ) -> int:
        """Count number of periods between two dates."""
        delta = end - start
        if period == "day":
            return max(1, delta.days)
        elif period == "week":
            return max(1, delta.days // 7)
        else:  # month
            return max(1, delta.days // 30)
```

**Step 3: Write tests**

```python
# backend/tests/services/test_dora_metrics.py
from datetime import datetime
from unittest.mock import AsyncMock, MagicMock

import pytest

from app.services.metrics.dora import DORAMetricsService


class TestDORAMetricsService:
    """Test DORA metrics calculations."""

    def test_group_by_period_day(self):
        service = DORAMetricsService(MagicMock())
        dates = [
            datetime(2026, 1, 15, 10, 0),
            datetime(2026, 1, 15, 14, 0),
            datetime(2026, 1, 16, 9, 0),
        ]
        
        result = service._group_by_period(dates, "day")
        
        assert len(result) == 2
        assert result[0] == {"period": "2026-01-15", "count": 2}
        assert result[1] == {"period": "2026-01-16", "count": 1}

    def test_group_by_period_week(self):
        service = DORAMetricsService(MagicMock())
        dates = [
            datetime(2026, 1, 13, 10, 0),  # Week 02
            datetime(2026, 1, 14, 10, 0),  # Week 02
            datetime(2026, 1, 20, 10, 0),  # Week 03
        ]
        
        result = service._group_by_period(dates, "week")
        
        assert len(result) == 2

    def test_count_periods(self):
        service = DORAMetricsService(MagicMock())
        start = datetime(2026, 1, 1)
        end = datetime(2026, 1, 31)
        
        assert service._count_periods(start, end, "day") == 30
        assert service._count_periods(start, end, "week") == 4
        assert service._count_periods(start, end, "month") == 1
```

**Step 4: Run tests**

Run: `cd backend && uv run pytest tests/services/test_dora_metrics.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add backend/app/services/metrics/ backend/tests/services/
git commit -m "feat(backend): add DORA metrics service

- Deployment Frequency: count deployments per period with average
- Lead Time for Changes: measure time from first commit to deployment"
```

---

## Task 7: Add Metrics API endpoints

**Files:**
- Create: `backend/app/api/v1/endpoints/metrics.py`
- Modify: `backend/app/api/v1/router.py`

**Step 1: Create metrics endpoints**

```python
# backend/app/api/v1/endpoints/metrics.py
from datetime import datetime, timedelta
from typing import Literal

from fastapi import APIRouter, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_db
from app.services.metrics.dora import DORAMetricsService

router = APIRouter(prefix="/metrics", tags=["metrics"])


def get_default_date_range() -> tuple[datetime, datetime]:
    """Default to last 30 days."""
    end = datetime.utcnow()
    start = end - timedelta(days=30)
    return start, end


@router.get("/dora")
async def get_dora_metrics(
    start_date: datetime | None = None,
    end_date: datetime | None = None,
    period: Literal["day", "week", "month"] = "week",
    repo_id: int | None = None,
    db: AsyncSession = Depends(get_db),
):
    """Get all DORA metrics."""
    if not start_date or not end_date:
        start_date, end_date = get_default_date_range()
    
    service = DORAMetricsService(db)
    
    deployment_freq = await service.get_deployment_frequency(
        start_date, end_date, period, repo_id
    )
    lead_time = await service.get_lead_time_for_changes(
        start_date, end_date, repo_id
    )
    
    return {
        "deployment_frequency": deployment_freq,
        "lead_time_for_changes": lead_time,
    }


@router.get("/dora/deployment-frequency")
async def get_deployment_frequency(
    start_date: datetime | None = None,
    end_date: datetime | None = None,
    period: Literal["day", "week", "month"] = "week",
    repo_id: int | None = None,
    db: AsyncSession = Depends(get_db),
):
    """Get deployment frequency metric."""
    if not start_date or not end_date:
        start_date, end_date = get_default_date_range()
    
    service = DORAMetricsService(db)
    return await service.get_deployment_frequency(start_date, end_date, period, repo_id)


@router.get("/dora/lead-time")
async def get_lead_time(
    start_date: datetime | None = None,
    end_date: datetime | None = None,
    repo_id: int | None = None,
    db: AsyncSession = Depends(get_db),
):
    """Get lead time for changes metric."""
    if not start_date or not end_date:
        start_date, end_date = get_default_date_range()
    
    service = DORAMetricsService(db)
    return await service.get_lead_time_for_changes(start_date, end_date, repo_id)
```

**Step 2: Register router**

Add to `backend/app/api/v1/router.py`:

```python
from app.api.v1.endpoints import metrics

api_router.include_router(metrics.router)
```

**Step 3: Commit**

```bash
git add backend/app/api/v1/
git commit -m "feat(backend): add DORA metrics API endpoints

- GET /metrics/dora - all DORA metrics
- GET /metrics/dora/deployment-frequency - deployment frequency
- GET /metrics/dora/lead-time - lead time for changes

Query params: start_date, end_date, period, repo_id"
```

---

## Task 8: Update connectors sync to include GitHub Actions

**Files:**
- Modify: `backend/app/api/v1/endpoints/connectors.py`
- Modify: `backend/app/services/scheduler.py`

**Step 1: Update connectors endpoint**

Modify `backend/app/api/v1/endpoints/connectors.py` to include GitHub Actions connector status and sync:

```python
from app.connectors.factory import create_github_connector, create_github_actions_connector

@router.get("/status", response_model=list[ConnectorStatus])
async def get_connectors_status():
    """Get status of all configured connectors."""
    statuses = []
    
    github = create_github_connector()
    if github:
        connected = await github.test_connection()
        statuses.append(ConnectorStatus(name="github", connected=connected))
        await github.close()
    
    github_actions = create_github_actions_connector()
    if github_actions:
        connected = await github_actions.test_connection()
        statuses.append(ConnectorStatus(name="github_actions", connected=connected))
        await github_actions.close()
    
    return statuses


@router.post("/sync", response_model=list[SyncResult])
async def trigger_sync(db: AsyncSession = Depends(get_db)):
    """Trigger full sync of all connectors."""
    results = []
    
    github = create_github_connector()
    if github:
        result = await github.sync_all(db)
        results.append(result)
        await github.close()
    
    github_actions = create_github_actions_connector()
    if github_actions:
        result = await github_actions.sync_all(db)
        results.append(result)
        await github_actions.close()
    
    return results
```

**Step 2: Update scheduler**

Add GitHub Actions to the scheduled sync in `backend/app/services/scheduler.py`.

**Step 3: Commit**

```bash
git add backend/app/api/v1/endpoints/connectors.py backend/app/services/scheduler.py
git commit -m "feat(backend): include GitHub Actions in connector status and sync"
```

---

## Task 9: Add fl_chart dependency to Flutter

**Files:**
- Modify: `frontend/pubspec.yaml`

**Step 1: Add fl_chart dependency**

Add to `frontend/pubspec.yaml` under dependencies:

```yaml
dependencies:
  flutter:
    sdk: flutter
  flutter_riverpod: ^2.4.0
  http: ^1.1.0
  fl_chart: ^0.66.0
```

**Step 2: Run pub get**

Run: `cd frontend && flutter pub get`

**Step 3: Commit**

```bash
git add frontend/pubspec.yaml frontend/pubspec.lock
git commit -m "build(frontend): add fl_chart for dashboard charts"
```

---

## Task 10: Create Flutter DORA Metrics Models

**Files:**
- Create: `frontend/lib/models/dora_metrics.dart`

**Step 1: Create Dart models**

```dart
// frontend/lib/models/dora_metrics.dart

class DeploymentFrequency {
  final String period;
  final String startDate;
  final String endDate;
  final List<PeriodData> data;
  final int total;
  final double average;

  DeploymentFrequency({
    required this.period,
    required this.startDate,
    required this.endDate,
    required this.data,
    required this.total,
    required this.average,
  });

  factory DeploymentFrequency.fromJson(Map<String, dynamic> json) {
    return DeploymentFrequency(
      period: json['period'] as String,
      startDate: json['start_date'] as String,
      endDate: json['end_date'] as String,
      data: (json['data'] as List)
          .map((e) => PeriodData.fromJson(e as Map<String, dynamic>))
          .toList(),
      total: json['total'] as int,
      average: (json['average'] as num).toDouble(),
    );
  }
}

class PeriodData {
  final String period;
  final int count;

  PeriodData({required this.period, required this.count});

  factory PeriodData.fromJson(Map<String, dynamic> json) {
    return PeriodData(
      period: json['period'] as String,
      count: json['count'] as int,
    );
  }
}

class LeadTimeForChanges {
  final String startDate;
  final String endDate;
  final int count;
  final double averageHours;
  final double medianHours;
  final List<LeadTimeMeasurement> measurements;

  LeadTimeForChanges({
    required this.startDate,
    required this.endDate,
    required this.count,
    required this.averageHours,
    required this.medianHours,
    required this.measurements,
  });

  factory LeadTimeForChanges.fromJson(Map<String, dynamic> json) {
    return LeadTimeForChanges(
      startDate: json['start_date'] as String,
      endDate: json['end_date'] as String,
      count: json['count'] as int,
      averageHours: (json['average_hours'] as num).toDouble(),
      medianHours: (json['median_hours'] as num).toDouble(),
      measurements: (json['measurements'] as List)
          .map((e) => LeadTimeMeasurement.fromJson(e as Map<String, dynamic>))
          .toList(),
    );
  }
}

class LeadTimeMeasurement {
  final int deploymentId;
  final String firstCommitAt;
  final String deployedAt;
  final double leadTimeHours;

  LeadTimeMeasurement({
    required this.deploymentId,
    required this.firstCommitAt,
    required this.deployedAt,
    required this.leadTimeHours,
  });

  factory LeadTimeMeasurement.fromJson(Map<String, dynamic> json) {
    return LeadTimeMeasurement(
      deploymentId: json['deployment_id'] as int,
      firstCommitAt: json['first_commit_at'] as String,
      deployedAt: json['deployed_at'] as String,
      leadTimeHours: (json['lead_time_hours'] as num).toDouble(),
    );
  }
}

class DORAMetrics {
  final DeploymentFrequency deploymentFrequency;
  final LeadTimeForChanges leadTimeForChanges;

  DORAMetrics({
    required this.deploymentFrequency,
    required this.leadTimeForChanges,
  });

  factory DORAMetrics.fromJson(Map<String, dynamic> json) {
    return DORAMetrics(
      deploymentFrequency: DeploymentFrequency.fromJson(
          json['deployment_frequency'] as Map<String, dynamic>),
      leadTimeForChanges: LeadTimeForChanges.fromJson(
          json['lead_time_for_changes'] as Map<String, dynamic>),
    );
  }
}
```

**Step 2: Commit**

```bash
git add frontend/lib/models/
git commit -m "feat(frontend): add DORA metrics Dart models"
```

---

## Task 11: Create Flutter Metrics Service

**Files:**
- Create: `frontend/lib/services/metrics_service.dart`
- Modify: `frontend/lib/services/providers.dart`

**Step 1: Create metrics service**

```dart
// frontend/lib/services/metrics_service.dart
import 'dart:convert';
import 'package:http/http.dart' as http;
import '../core/config.dart';
import '../models/dora_metrics.dart';

class MetricsService {
  final String baseUrl;

  MetricsService({String? baseUrl}) : baseUrl = baseUrl ?? AppConfig.apiBaseUrl;

  Future<DORAMetrics> getDORAMetrics({
    DateTime? startDate,
    DateTime? endDate,
    String period = 'week',
    int? repoId,
  }) async {
    final queryParams = <String, String>{
      'period': period,
    };
    
    if (startDate != null) {
      queryParams['start_date'] = startDate.toIso8601String();
    }
    if (endDate != null) {
      queryParams['end_date'] = endDate.toIso8601String();
    }
    if (repoId != null) {
      queryParams['repo_id'] = repoId.toString();
    }

    final uri = Uri.parse('$baseUrl/metrics/dora').replace(queryParameters: queryParams);
    final response = await http.get(uri);

    if (response.statusCode == 200) {
      return DORAMetrics.fromJson(json.decode(response.body));
    } else {
      throw Exception('Failed to load DORA metrics');
    }
  }

  Future<DeploymentFrequency> getDeploymentFrequency({
    DateTime? startDate,
    DateTime? endDate,
    String period = 'week',
  }) async {
    final queryParams = <String, String>{'period': period};
    
    if (startDate != null) {
      queryParams['start_date'] = startDate.toIso8601String();
    }
    if (endDate != null) {
      queryParams['end_date'] = endDate.toIso8601String();
    }

    final uri = Uri.parse('$baseUrl/metrics/dora/deployment-frequency')
        .replace(queryParameters: queryParams);
    final response = await http.get(uri);

    if (response.statusCode == 200) {
      return DeploymentFrequency.fromJson(json.decode(response.body));
    } else {
      throw Exception('Failed to load deployment frequency');
    }
  }
}
```

**Step 2: Add provider**

Update `frontend/lib/services/providers.dart`:

```dart
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'metrics_service.dart';

final metricsServiceProvider = Provider<MetricsService>((ref) {
  return MetricsService();
});
```

**Step 3: Commit**

```bash
git add frontend/lib/services/
git commit -m "feat(frontend): add MetricsService for DORA API calls"
```

---

## Task 12: Create KPI Card Widget

**Files:**
- Create: `frontend/lib/widgets/kpi_card.dart`

**Step 1: Create reusable KPI card widget**

```dart
// frontend/lib/widgets/kpi_card.dart
import 'package:flutter/material.dart';

class KPICard extends StatelessWidget {
  final String title;
  final String value;
  final String? subtitle;
  final IconData icon;
  final Color? color;

  const KPICard({
    super.key,
    required this.title,
    required this.value,
    this.subtitle,
    required this.icon,
    this.color,
  });

  @override
  Widget build(BuildContext context) {
    final cardColor = color ?? Theme.of(context).primaryColor;
    
    return Card(
      elevation: 2,
      child: Padding(
        padding: const EdgeInsets.all(16.0),
        child: Column(
          crossAxisAlignment: CrossAxisAlignment.start,
          children: [
            Row(
              children: [
                Icon(icon, color: cardColor, size: 24),
                const SizedBox(width: 8),
                Expanded(
                  child: Text(
                    title,
                    style: Theme.of(context).textTheme.titleSmall?.copyWith(
                          color: Colors.grey[600],
                        ),
                  ),
                ),
              ],
            ),
            const SizedBox(height: 12),
            Text(
              value,
              style: Theme.of(context).textTheme.headlineMedium?.copyWith(
                    fontWeight: FontWeight.bold,
                    color: cardColor,
                  ),
            ),
            if (subtitle != null) ...[
              const SizedBox(height: 4),
              Text(
                subtitle!,
                style: Theme.of(context).textTheme.bodySmall?.copyWith(
                      color: Colors.grey[500],
                    ),
              ),
            ],
          ],
        ),
      ),
    );
  }
}
```

**Step 2: Commit**

```bash
git add frontend/lib/widgets/
git commit -m "feat(frontend): add KPICard widget for dashboard metrics"
```

---

## Task 13: Create Dashboard Screen

**Files:**
- Create: `frontend/lib/screens/dashboard_screen.dart`
- Modify: `frontend/lib/main.dart`

**Step 1: Create dashboard screen**

```dart
// frontend/lib/screens/dashboard_screen.dart
import 'package:flutter/material.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import '../models/dora_metrics.dart';
import '../services/metrics_service.dart';
import '../services/providers.dart';
import '../widgets/kpi_card.dart';

final doraMetricsProvider = FutureProvider<DORAMetrics>((ref) async {
  final service = ref.read(metricsServiceProvider);
  return service.getDORAMetrics();
});

class DashboardScreen extends ConsumerWidget {
  const DashboardScreen({super.key});

  @override
  Widget build(BuildContext context, WidgetRef ref) {
    final metricsAsync = ref.watch(doraMetricsProvider);

    return Scaffold(
      appBar: AppBar(
        title: const Text('Veelocity Dashboard'),
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () => ref.refresh(doraMetricsProvider),
          ),
        ],
      ),
      body: metricsAsync.when(
        loading: () => const Center(child: CircularProgressIndicator()),
        error: (error, stack) => Center(
          child: Column(
            mainAxisAlignment: MainAxisAlignment.center,
            children: [
              const Icon(Icons.error_outline, size: 48, color: Colors.red),
              const SizedBox(height: 16),
              Text('Error loading metrics: $error'),
              const SizedBox(height: 16),
              ElevatedButton(
                onPressed: () => ref.refresh(doraMetricsProvider),
                child: const Text('Retry'),
              ),
            ],
          ),
        ),
        data: (metrics) => _buildDashboard(context, metrics),
      ),
    );
  }

  Widget _buildDashboard(BuildContext context, DORAMetrics metrics) {
    return SingleChildScrollView(
      padding: const EdgeInsets.all(16.0),
      child: Column(
        crossAxisAlignment: CrossAxisAlignment.start,
        children: [
          Text(
            'DORA Metrics',
            style: Theme.of(context).textTheme.headlineSmall,
          ),
          const SizedBox(height: 16),
          LayoutBuilder(
            builder: (context, constraints) {
              final isWide = constraints.maxWidth > 600;
              return Wrap(
                spacing: 16,
                runSpacing: 16,
                children: [
                  SizedBox(
                    width: isWide ? (constraints.maxWidth - 16) / 2 : constraints.maxWidth,
                    child: KPICard(
                      title: 'Deployment Frequency',
                      value: '${metrics.deploymentFrequency.average}/week',
                      subtitle: '${metrics.deploymentFrequency.total} total deployments',
                      icon: Icons.rocket_launch,
                      color: Colors.blue,
                    ),
                  ),
                  SizedBox(
                    width: isWide ? (constraints.maxWidth - 16) / 2 : constraints.maxWidth,
                    child: KPICard(
                      title: 'Lead Time for Changes',
                      value: _formatLeadTime(metrics.leadTimeForChanges.averageHours),
                      subtitle: 'Median: ${_formatLeadTime(metrics.leadTimeForChanges.medianHours)}',
                      icon: Icons.timer,
                      color: Colors.green,
                    ),
                  ),
                ],
              );
            },
          ),
          const SizedBox(height: 32),
          Text(
            'Recent Deployments',
            style: Theme.of(context).textTheme.titleLarge,
          ),
          const SizedBox(height: 8),
          _buildDeploymentsList(metrics.deploymentFrequency.data),
        ],
      ),
    );
  }

  String _formatLeadTime(double hours) {
    if (hours < 1) {
      return '${(hours * 60).round()} min';
    } else if (hours < 24) {
      return '${hours.toStringAsFixed(1)} hrs';
    } else {
      return '${(hours / 24).toStringAsFixed(1)} days';
    }
  }

  Widget _buildDeploymentsList(List<PeriodData> data) {
    if (data.isEmpty) {
      return const Card(
        child: Padding(
          padding: EdgeInsets.all(16.0),
          child: Text('No deployments in this period'),
        ),
      );
    }

    return Card(
      child: ListView.separated(
        shrinkWrap: true,
        physics: const NeverScrollableScrollPhysics(),
        itemCount: data.length,
        separatorBuilder: (_, __) => const Divider(height: 1),
        itemBuilder: (context, index) {
          final item = data[index];
          return ListTile(
            leading: const Icon(Icons.check_circle, color: Colors.green),
            title: Text(item.period),
            trailing: Text(
              '${item.count} deployments',
              style: const TextStyle(fontWeight: FontWeight.bold),
            ),
          );
        },
      ),
    );
  }
}
```

**Step 2: Update main.dart**

Replace the home screen with DashboardScreen:

```dart
// frontend/lib/main.dart
import 'package:flutter/material.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'screens/dashboard_screen.dart';

void main() {
  runApp(const ProviderScope(child: VeelocityApp()));
}

class VeelocityApp extends StatelessWidget {
  const VeelocityApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Veelocity',
      theme: ThemeData(
        colorScheme: ColorScheme.fromSeed(seedColor: Colors.indigo),
        useMaterial3: true,
      ),
      home: const DashboardScreen(),
    );
  }
}
```

**Step 3: Run the app**

Run: `cd frontend && flutter run -d chrome`

**Step 4: Commit**

```bash
git add frontend/lib/
git commit -m "feat(frontend): add basic dashboard with DORA metrics KPI cards

- Dashboard screen with deployment frequency and lead time
- Responsive layout for mobile and desktop
- Error handling and refresh capability"
```

---

## Task 14: Final integration test

**Step 1: Start backend**

Run: `make dev-local`

**Step 2: Trigger sync**

```bash
curl -X POST http://localhost:8000/api/v1/connectors/sync
```

**Step 3: Verify metrics**

```bash
curl http://localhost:8000/api/v1/metrics/dora
```

**Step 4: Run Flutter app**

Run: `cd frontend && flutter run -d chrome`

Verify dashboard displays DORA metrics.

**Step 5: Commit final state**

```bash
git add .
git commit -m "feat: complete v0.3 GitHub Actions and DORA metrics integration"
```

---

## Summary

v0.3 delivers:

1. **GitHub Actions Models**: Workflow, WorkflowRun with is_deployment flag
2. **GitHubActionsConnector**: Fetches workflows and runs from GitHub API
3. **DORA Metrics Service**: Calculates Deployment Frequency and Lead Time
4. **Metrics API**: `/metrics/dora`, `/metrics/dora/deployment-frequency`, `/metrics/dora/lead-time`
5. **Flutter Dashboard**: KPI cards showing DORA metrics with responsive layout

Next version (v0.4) will add Linear integration and development metrics (PR Review Time, Cycle Time, Throughput).
